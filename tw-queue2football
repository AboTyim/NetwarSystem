#!/usr/bin/python
# drain Redis queue of numeric IDs, populating
# usertest index.
import tweepy
import os, sys, json, time, psutil, setproctitle
import configparser, getpass, platform
from elasticsearch import Elasticsearch
from walrus import *
import ssl, requests
from elasticsearch.connection import create_ssl_context
import squish2

def perflog(scname,mesg):
	mydate = str(datetime.datetime.utcnow())
	ebod = "{ \"index\" : { \"_index\" : \"perflog\", \"_type\" : \"perfdata\"} }\n"
	ebod = ebod + "{\"host\" : \"" + platform.node() + "\", \"shell\" : \"" + getpass.getuser() + "\", \"screen_name\" : \"" 
	ebod = ebod + str(scname) + "\", \"event\" : \"" + str(mesg) + "\" , \"date\" : \"" + mydate + "\"}\n"
	#print(ebod)
	try:
		client.bulk(body=ebod)
	except(RuntimeError, TypeError, NameError):
		pass
def check_resources():
	# make sure prior instance has exited
	for p in psutil.process_iter(attrs=['name', 'username']):
		if (p.info['username'] == getpass.getuser()) and (p.info['name'] == "campfollowers" +getpass.getuser()):
			perflog("FAIL", "already running")
			sys.exit()

	setproctitle.setproctitle("work" + getpass.getuser())


# start here
requests.packages.urllib3.disable_warnings()


check_resources()
config = configparser.ConfigParser()
config.read(os.environ['HOME'] +'/.twitter')
auth = tweepy.OAuthHandler(config['API']['consumer_key'], config['API']['consumer_secret'])
auth.set_access_token(config['API']['access_token_key'], config['API']['access_token_secret'])
api = tweepy.API(auth)

if(config['STREAM']['elksg'][:5] == "https"):
	print(config['STREAM']['elksg'][:5])
	try:
		ssl_context = create_ssl_context(cafile='/home/root-ca.pem')
		ssl_context.check_hostname = False
		ssl_context.verify_mode = ssl.CERT_NONE
		client = Elasticsearch(config['STREAM']['elksg'], ssl_context=ssl_context, timeout=60, http_auth=(config['STREAM']['elksguser'], config['STREAM']['elksgpass']))
	except:
		sys.exit()
else:
	client = Elasticsearch(config['STREAM']['elksg'])

client.indices.create(index='campfollowers', ignore=400)
wal = Walrus(host=config['STREAM']['redishost'],port=config['STREAM']['redisport'], db=0)
tum = wal.Set("camps")


twbod = ""
cnt = 0
cnt2 = 0
popcnt = 0
while(cnt2 < 800):
	try:
		k = tum.pop().decode('utf-8')
		print(k)
		y = api.get_user(k)
		popcnt = popcnt + 1
		if y:
			id = y._json.pop('id', None)
			y = squish2.squishuser(y)
			print(y.screen_name)
			twbod = twbod + "{ \"index\" : { \"_index\" : \"campfollowers\", \"_type\" : \"userid\", \"_id\" : \"" + y.id_str + "\" }\n"
			twbod = twbod + json.dumps(y._json) + "\n"
			cnt = cnt + 1
	except (RuntimeError, TypeError, NameError, tweepy.error.TweepError):
		perflog(config['API']['account'], "tweepy error " + str(tweepy.error.TweepError))
		pass
	except(AttributeError):
		perflog(config['API']['account'], "AttributeError " + str(id))
		cnt = 800
	# stash every 200 userids collected.
	if(cnt > 199):
		print("cnt > 199")
		client.bulk(index="campfollowers",doc_type="userid",body=twbod)
		cnt2 = cnt2 + cnt
		twbod = ""
		cnt = 0
		perflog(config['API']['account'],"test profiling " + str(cnt2) + " popcnt " + str(popcnt))
		zod = api.rate_limit_status()
		# API provides 900 of these per every fifteen minutes. When we notice
		# fewer than 100 calls left, we quit. The accounts used to do this
		# may be doing other things where they need some calls, this seems to
		# be enough cushion to keep them from failing.
		if(zod['resources']['users']['/users/show/:id']['remaining'] < 100):
			cnt2 = 800

