#!/usr/bin/python
import csv, json, os, random, re, sys, time, redis, tweepy, getpass
import configparser
from elasticsearch import Elasticsearch
from elasticsearch_dsl import Search
from walrus import *
import psutil, getpass, setproctitle, platform
import logging, logging.handlers
from time import gmtime, strftime
import ssl
from elasticsearch.connection import create_ssl_context
import squish2

def perflog(scname,mesg):
        mydate = str(datetime.datetime.utcnow())   
        ebod = "{ \"index\" : { \"_index\" : \"perflog\", \"_type\" : \"perfdata\"} }\n"
        ebod = ebod + "{\"host\" : \"" + platform.node() + "\", \"shell\" : \"" + getpass.getuser() + "\", \"screen_name\" : \""
        ebod = ebod + scname + "\", \"event\" : \"" + mesg + "\" , \"date\" : \"" + mydate + "\"}\n"
        #print(ebod)
        try:
                client.bulk(body=ebod)
        except(RuntimeError, TypeError, NameError):
                pass



def check_resources():
	# make sure prior instance has exited. This prevents
	# multiple instances running when there are accounts
	# with 3200+ tweets being collected. One account can 
	# take up to 90 seconds due to API even under otherwise
	# perfect conditions.
	for p in psutil.process_iter(attrs=['name', 'username']):
		if (p.info['username'] == getpass.getuser()) and (p.info['name'] == "work" +getpass.getuser()):
			perflog("FAIL", "already running")
			sys.exit()

	setproctitle.setproctitle("work" + getpass.getuser())

	i = 0
	# Check system resources at two second intervals for thirty
	# seconds. This smooths load when large numbers of processes
	# launch via cron. Prior version of code wired maxcpu to 75
	# and maxmem to 92. These numbers made sense on a machine with
	# 16 cores, 128 gig of ram, and no other duties except running
	# 64 accounts in parallel. There isn't much harm in delaying
	# automated account following for a minute here and there, this
	# is the first place to trim system load.
	while (i < 30):
		zcpu = psutil.cpu_percent(interval=None, percpu=False)
		zmem = psutil.virtual_memory()[2]
		if (zcpu < config['API']['maxcpu']) and (zmem < config['API']['maxmem']):
			i = 60
		i = i + 2
		time.sleep(2)

	#i < 60, don't have resources
	if (i  < 60):
		print()
		perflog("FAIL","zcpu " + str(zcpu) + " zmem " + str(zmem))
		sys.exit()
	else:
		print("")
		#perflog(getpass.getuser(),"sufficient resources to run")

##########################################################
# Because people scratch their heads over this algorithm.
#
# There are three Walrus sets - ADDS, SEEN, and WORK,
# which provide a modified round robin processing flow
# that prioritizes newly listed accounts for collection.
#
# When system is started OR when it has emptied ADDS & WORK,
# all three sets are emptied. The system runs for the first
# time, placing account IDs in ADDS, the priority processing
# queue, and SEEN. During future invocations any account on
# a list but not in SEEN must have been freshly listed, so
# it gets queued to be processed first.
#
# When an account ID is found in ADDS, it gets popped from
# that set, queued for immediate processing, and it is added 
# to WORK, which will ensure it gets collected as part of 
# the normal flow.
#
# When ADDS has been emptied, the system begins popping 
# accounts from WORK. When WORK is exhausted, the system
# treats this condition just like it has been restarted.
#
# Normal execution means that all listed accounts get
# processed once because they were found in ADDS, then again
# because they were found in WORK, then the system restarts.
# Any accounts added to a list at any point during the run
# are given priority. This is done so an operator tracking
# 480 accounts does not have to wait eight hours before a
# single account they have added appears in the system.
#
# This algorithm has a 1 in (total accounts tracked) chance
# of finding a newly listed account during a reload, which
# will introduce an average delay before that account
# becomes available of (total accounts tracked)/2 minutes.
#
# During most processing time, defined by:
#
# (total accounts tracked - 1)/(total accounts tracked)
#
# The longest possible delay is sixty seconds for the 
# currently executing account to finish, plus ninety 
# seconds for the system to collect 3,200 tweets from a 
# new account.
#
# There are faster ways to do this, but they all require
# the system to have memory of the past. This method is
# perceived as fast by the collection account operator and
# its behavior is entirely controlled by the lists in 
# the account they use. Any restart/failover of a server
# in a clustered system passes unnoticed.
#
##########################################################
def load_lists(myname):
	lists = api.lists_all(myname)
	if(len(work) == 0 ):
		seen.clear()
	for list in lists:
		print(list.slug)
		if re.match("nsce-", list.slug):
			for member in tweepy.Cursor(api.list_members,myname,list.slug).items():
				if(member.screen_name not in seen):
					#print("adding " + list.slug + " " +  member.screen_name)
					seen.add(member.screen_name)
					adds.add(member.screen_name)

def get_all_tweets(acct_name):
	##########################################################
	# Each index of tweets has a companion index of Twitter ID
	# data that contains the ID of the last status emitted by 
	# the account. When the system processes an account it is
	# either previously unseen, which means we collect up to
	# 3,200 tweets, or it has been seen, in which case we only
	# want the newly added tweets.
	# 
	# This method is admittedly imperfect. If an account is
	# collected, then it deletes its final tweet, the system
	# will collect up to 3,200 tweets again the next time it
	# is processed. We use Twitter's "snowflake" numbers as 
	# the unique identifier for both tweets and user ID data, 
	# which means there is no way content would be duplicated,
	# but this does waste time and API resources when it
	# happens.
	#
	# It might make more sense to query the index, retrieving
	# the last tweet from an account, then using either its
	# ID or time stamp to limit processing. Testing this is
	# on our short list of development goals.
	##########################################################

	maxtweet = "9999999999999999999"
	if client.indices.exists(index="tu" + getpass.getuser()):
		s = Search(using=client, index="tu" + getpass.getuser()).query("match", screen_name=acct_name)
		resp = s.execute()
		if(resp):
			maxtweet = resp[0].status.id_str
	bod = ""
	cnt1 = 0
	cnt2 = 0
	for tweet in tweepy.Cursor(api.user_timeline, id=acct_name,tweet_mode='extended').items():
		cnt1 = cnt1 + 1
		cnt2 = cnt2 + 1
		if(tweet.id_str == maxtweet):
			print(maxtweet + " has been seen!!!")
			break
		print("adding " + acct_name + " " + str(tweet.id) + " " + " " + str(cnt2))
		bod = bod + "{ \"index\" : { \"_index\" : \"tw" + getpass.getuser() + "\", \"_type\" : \"tweets\", \"_id\" : \"" + str(tweet.id) + "\"} }\n"
		bod = bod + json.dumps(tweet._json) + "\n"
		if(cnt1 == 800):
			try:
				client.bulk(index="tw" + getpass.getuser(),doc_type="tweets",body=bod)
				bod = ""
				cnt1 = 0
			except (RuntimeError, TypeError, NameError):
				pass
	# leftovers
	if(len(bod) > 0):
		try:
			client.bulk(index="tw" + getpass.getuser(),doc_type="tweets",body=bod)
		except (RuntimeError, TypeError, NameError):
			pass

# main begin
config = configparser.ConfigParser()
config.read(os.environ['HOME'] +'/.twitter')

# check_resources
check_resources()

auth = tweepy.OAuthHandler(config['API']['consumer_key'], config['API']['consumer_secret'])
auth.set_access_token(config['API']['access_token_key'], config['API']['access_token_secret'])
api = tweepy.API(auth)
my = api.verify_credentials()

if(config['STREAM']['elksg'][:5] == "https"):
	print(config['STREAM']['elksg'][:5])
	try:
		ssl_context = create_ssl_context(cafile='/home/root-ca.pem')
		ssl_context.check_hostname = False
		ssl_context.verify_mode = ssl.CERT_NONE
		client = Elasticsearch(config['STREAM']['elksg'], ssl_context=ssl_context, timeout=60, http_auth=(config['STREAM']['elksguser'], config['STREAM']['elksgpass']))
	except:
		sys.exit()
else:
	client = Elasticsearch(config['STREAM']['elksg'])

wal = Walrus(host='localhost', port=6379, db=0)
adds = wal.Set(getpass.getuser() + "adds")
work = wal.Set(getpass.getuser() + "work")
seen = wal.Set(getpass.getuser() + "seen")


load_lists(my.screen_name)

if(len(adds) > 0):
	nextid = adds.pop().decode('utf-8')
	work.add(nextid)
	print("adds: " + str(nextid))
else:
	nextid = work.pop().decode('utf-8')
	print("work: " + str(nextid))

get_all_tweets(nextid)
perflog("monitoring",nextid)
try:
	y = api.get_user(nextid)
	if y:
		y = squish2.squishuser(y)
		twbod = "{ \"index\" : { \"_index\" : \"tu" + getpass.getuser() + "\", \"_type\" : \"userid\", \"_id\" : \"" + y.id_str + "\" }\n"
		twbod = twbod + json.dumps(y._json)
		client.bulk(index="tu" + getpass.getuser(),doc_type="userid",body=twbod)
except (RuntimeError, TypeError, NameError):
	print("tu index write failed failed")
	pass

try:
#	y = api.get_user(nextid)
	if y:
		y = squish2.squishuser(y)
		twbod = "{ \"index\" : { \"_index\" : \"usertest\", \"_type\" : \"userid\", \"_id\" : \"" + y.id_str + "\" }\n"
		twbod = twbod + json.dumps(y._json)
		client.bulk(index="tumaster",doc_type="userid",body=twbod)
except (RuntimeError, TypeError, NameError):
	print("usertest index write failed")
	pass
