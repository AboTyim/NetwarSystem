#!/usr/bin/python
import tweepy
import os, sys, json, time, configparser
import csv, random, re, getpass
from elasticsearch import Elasticsearch
from elasticsearch_dsl import Search
import ssl
from elasticsearch.connection import create_ssl_context
import redis
from walrus import *
import squish2, requests

requests.packages.urllib3.disable_warnings()

config = configparser.ConfigParser()
config.read(os.environ['HOME'] +'/.twitter')
sysconf = configparser.ConfigParser()
sysconf.read('/etc/netwar/netwar.conf')

if(sysconf['SYSTEM']['elksg'][:5] == "https"):
        try:
                ssl_context = create_ssl_context(cafile='/home/root-ca.pem')
                ssl_context.check_hostname = False
                ssl_context.verify_mode = ssl.CERT_NONE
                client = Elasticsearch(sysconf['SYSTEM']['elksg'], ssl_context=ssl_context, timeout=60, http_auth=(sysconf['SYSTEM']['elksguser'], sysconf['SYSTEM']['elksgpass']))
        except:
                sys.exit()

auth = tweepy.OAuthHandler(config['API']['consumer_key'], config['API']['consumer_secret'])
auth.set_access_token(config['API']['access_token_key'], config['API']['access_token_secret'])
api = tweepy.API(auth)


if(len(sys.argv) < 3):
	print("tw-walgetfollowers <index> <screen_name>")
	sys.exit()

wal = Walrus(host=sysconf['SYSTEM']['redishost'],port=sysconf['SYSTEM']['redisport'], db=0)
qq = wal.Set(sys.argv[1])

#https://sarahleejane.github.io/learning/python/2015/10/14/creating-an-elastic-search-index-with-python.html
#	request_body = {
#	    "settings" : {
#	        "number_of_shards": 5,
#	        "number_of_replicas": 1
#	    },
#
#	    'mappings': {
#	        'examplecase': {
#	            'properties': {
#	                'address': {'index': 'not_analyzed', 'type': 'string'},
#	                'date_of_birth': {'index': 'not_analyzed', 'format': 'dateOptionalTime', 'type': 'date'},
#	                'some_PK': {'index': 'not_analyzed', 'type': 'string'},
#	                'fave_colour': {'index': 'analyzed', 'type': 'string'},
#	                'email_domain': {'index': 'not_analyzed', 'type': 'string'},
#	            }}}
#	}

request_body = {
        "settings" : { "number_of_shards" : 3, "number_of_replicas": 1},
       'mappings' : {'userid' : { 'properties' : {
          'created_at' : {'type' : 'date',  'format' : 'EEE MMM dd HH:mm:ss Z YYYY'},
          'status_at' :  {'type' : 'date',  'format' : 'EEE MMM dd HH:mm:ss Z YYYY'},
          'collected_at' :  {'type' : 'date',  'format' : 'EEE MMM dd HH:mm:ss Z YYYY'},
          'coordinates' : {'properties' : {'coordinates' : {'type' : 'geo_point'},'type' : {'type' : 'text'}}},
          'id_str' : {'type' : 'keyword'},
          'lang' :   {'type' : 'keyword'},
          'source' : {'type' : 'keyword'},
          'text' :   {'type' : 'text'},
          'user' :   {'properties' : {'created_at' : {'type' : 'date',  'format' : 'EEE MMM dd HH:mm:ss Z YYYY'},
                                      'description' : {'type' : 'text'},
                                      'name' :        {'type' : 'text'},
                                      'screen_name' : {'type' : 'text'}}}}}}}

client.indices.create(index=sys.argv[1], body = request_body, ignore=400)



if True:
	try:
		for page in tweepy.Cursor(api.followers_ids, screen_name=sys.argv[2]).pages():
			for acct in page:
				s = Search(using=client, index=sys.argv[1]).query("match",id_str=str(acct))
				response = s.execute()
				if(response):
					for resp in response:
						print("located  " + resp['screen_name'])
				else:
					print("queueing " + str(acct))
					qq.add(str(acct))

			zod = api.rate_limit_status()
			while(zod['resources']['followers']['/followers/ids']['remaining'] < 1):
				time.sleep(60)
				zod = api.rate_limit_status()
	except (RuntimeError, TypeError, NameError):
		pass

