#!/usr/bin/python
# one shot bulk Twitter account recorder
import csv, json, os, random, re, sys, time, datetime
import urllib2, redis, tweepy
import psutil, getpass, setproctitle
import logging, logging.handlers
from simpleconfigparser import simpleconfigparser
from bs4 import BeautifulSoup
from time import gmtime, strftime
from elasticsearch import Elasticsearch
from elasticsearch_dsl import Search
from walrus import *

def elog(acct, mesg):
        mydate = str(datetime.datetime.utcnow())
        bod = "{ \"index\" : { \"_index\" : \"perflog\", \"_type\" : \"perfdata\"} }\n"
        bod = bod + "{\"name\" : \"" + acct + "\", \"event\" : \"" + mesg + "\" , \"date\" : \"" + mydate + "\"}\n"
        try:
                client.bulk(body=bod)
        except(RuntimeError, TypeError, NameError):
                pass

def get_all_tweets(screen_name):
	done = False
	cnt = 0
	bod = ""
	new_tweets = api.user_timeline(screen_name = screen_name,count=20)
	while(len(new_tweets) > 0) and not done:
		for tweet in new_tweets:
			delta = datetime.datetime.now() - tweet.created_at
			#print(delta.total_seconds())
			if(delta.total_seconds() > 604800):
				done = True
			else:
				cnt = cnt + 1
				print(tweet.id)
				bod = bod + "{ \"index\" : { \"_index\" : \"val2018\", \"_type\" : \"tweets\" } }\n"
				bod = bod + json.dumps(tweet._json) + "\n"
			try:
				if(len(bod) > 0):
					client.bulk(index="val2018",doc_type="tweets",body=bod)
					elog(getpass.getuser(), "successful bulk add " + str(len(bod)))
			except (RuntimeError, TypeError, NameError):
				elog(getpass.getuser(), "failed bulk add " + str(len(bod)))
				pass
			bod = ""
                new_tweets = api.user_timeline(screen_name = screen_name,count=50, max_id=tweet.id)
# get_all_tweets
def check_resources():
	# make sure prior instance has exited
	for p in psutil.process_iter(attrs=['name', 'username']):
        	if (p.info['username'] == getpass.getuser()) and (p.info['name'] == "work" +getpass.getuser()):
                	elog(getpass.getuser(), "already running")
			sys.exit()

	setproctitle.setproctitle("work" + getpass.getuser())
	time.sleep(random.random() * 20)

	i = 0
	while (i < 30):
		zcpu = psutil.cpu_percent(interval=None, percpu=False)
		zmem = psutil.virtual_memory()[2]
		if (zcpu < 75) and (zmem < 75):
			i = 60
		i = i + 2
		time.sleep(2)

	#i < 60, don't have resources
        if (i  < 60):
                print()
                elog(getpass.getuser(),"zcpu " + str(zcpu) + " zmem " + str(zmem))
                sys.exit()
        else:
                elog(getpass.getuser(),"sufficient resources to run")

# check_resources

# main begin
config = simpleconfigparser()
config.read(os.environ['HOME'] +'/.twitter')

try:
	client = Elasticsearch()
except:
	sys.exit()

#ensure prior instance has finished, and CPU/MEM usage < 75%
check_resources()


auth = tweepy.OAuthHandler(config.API.consumer_key, config.API.consumer_secret)
auth.set_access_token(config.API.access_token_key, config.API.access_token_secret)
api = tweepy.API(auth)
my = api.verify_credentials()
listlim = api.rate_limit_status()[u'resources'][u'lists'][u'/lists/members'][u'remaining']
tweetlim = api.rate_limit_status()[u'resources'][u'statuses'][u'/statuses/user_timeline'][u'remaining']
elog(getpass.getuser(), "twitter "  + my.screen_name + " tweetlim " + str(tweetlim) + " listlim " + str(listlim))

if(sys.argv[1]):
	get_all_tweets(sys.argv[1])
	elog(getpass.getuser(), "starting on " + sys.argv[1])
else:
	wal = Walrus(host='localhost', port=6379, db=0)
	work = wal.Set(getpass.getuser() + "work")
	nextacct = work.pop()
	elog(getpass.getuser(), "starting on " + nextacct)
	get_all_tweets(nextacct)
