#!/usr/bin/python
import csv, json, os, random, re, sys, time, redis, tweepy
from simpleconfigparser import simpleconfigparser
from elasticsearch import Elasticsearch
from elasticsearch_dsl import Search
from walrus import *

def load_lists(myname):
	lists = api.lists_all(myname)
	if(len(work) == 0 ):
		seen.clear()
	for list in lists:
		if re.match("r-", list.slug):
			for member in tweepy.Cursor(api.list_members,name,list.slug).items():
				if(member not in seen):
					seen.add(member)
					adds.add(member)

def get_all_tweets(screen_name):
	new_tweets = api.user_timeline(screen_name = screen_name,count=20)
	while(len(new_tweets) > 0):
		for tweet in new_tweets:
			bod = "{ \"index\" : { \"_index\" : \"test\", \"_type\" : \"tweets\" } }\n"
			bod = bod + json.dumps(tweet._json) + "\n"
			try:
				client.bulk(index="test",doc_type="tweets",body=bod)
			except (RuntimeError, TypeError, NameError):
				pass
		oldest = new_tweets[-1].id - 1
		#wasting 200 calls? How do this efficiently? Count tweets processed?
		new_tweets = api.user_timeline(screen_name = screen_name,count=200, max_id=tweet.id)
		if(oldest == new_tweets[-1].id - 1):
			new_tweets = []

# get_all_tweets

# main begin
config = simpleconfigparser()
config.read(os.environ['HOME'] +'/.twitter')

auth = tweepy.OAuthHandler(config.API.consumer_key, config.API.consumer_secret)
auth.set_access_token(config.API.access_token_key, config.API.access_token_secret)
api = tweepy.API(auth)
my = api.verify_credentials()

client = Elasticsearch()

wal = Walrus(host='localhost', port=6379, db=0)
adds = wal.Set(my.screen_name + "adds")
work = wal.Set(my.screen_name + "work")
seen = wal.Set(my.screen_name + "seen")



load_lists(my.screen_name)

nextid = adds.pop()

if(nextid):
	work.add(nextid)
else:
	nextid = work.pop()

if(nextid):
	get_all_tweets(nextid)
