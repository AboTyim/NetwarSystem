#!/usr/bin/python
# one shot bulk Twitter account recorder
import csv, json, os, random, re, sys, time, datetime
import redis, tweepy
import psutil, getpass, setproctitle, platform
import logging, logging.handlers
import configparser
from time import gmtime, strftime
from elasticsearch import Elasticsearch
from elasticsearch_dsl import Search
import ssl
from elasticsearch.connection import create_ssl_context
from walrus import *
import squish

def elog(scname,mesg):
	mydate = str(datetime.datetime.utcnow())
	ebod = "{ \"index\" : { \"_index\" : \"perflog\", \"_type\" : \"perfdata\"} }\n"
	ebod = ebod + "{\"host\" : \"" + platform.node() + "\", \"shell\" : \"" + getpass.getuser() + "\", \"screen_name\" : \"" 
	ebod = ebod + str(scname) + "\", \"event\" : \"" + str(mesg) + "\" , \"date\" : \"" + mydate + "\"}\n"
	#print(ebod)
	try:
		client.bulk(body=ebod)
	except(RuntimeError, TypeError, NameError):
		pass


def get_all_tweets(screen_name):
	cnt = 0
	bod = ""
	new_tweets = api.user_timeline(screen_name = screen_name,count=20)
	while(len(new_tweets) > 0):
		for tweet in new_tweets:
			cnt = cnt + 1
			print(str(tweet.id) + " " + str(screen_name) + " " + str(cnt))
			bod = bod + "{ \"index\" : { \"_index\" : \"tw" + sys.argv[1] + "\", \"_type\" : \"tweets\", \"_id\" : \"" + str(tweet.id) + "\"} }\n"
			tweet._json.pop('coordinates', None)
			tweet._json.pop('contributors', None)
			tweet._json.pop('is_quote_status', None)
			tweet._json.pop('in_reply_to_status_id', None)
			tweet._json.pop('favorite_count', None)
			tweet._json.pop('in_reply_to_screen_name', None)
			tweet._json.pop('in_reply_to_user_id', None)
			tweet._json.pop('retweet_count', None)
			tweet._json.pop('favorite', None)
			tweet._json.pop('favorited', None)
			tweet._json.pop('favorite_count', None)
			tweet._json.pop('in_reply_to_user_id_str', None)
			tweet._json.pop('possibly_sensitive', None)
			tweet._json.pop('in_reply_to_status_id_str', None)
			tweet._json.pop('quoted_status', None)
			tweet._json.pop('quoted_status_id', None)
			tweet._json.pop('quoted_status_id_str', None)
			tweet._json.pop('retweeted', None)
			tweet._json.pop('retweeted_status', None)
			tweet._json.pop('retweets', None)
			tweet._json.pop('retweet', None)
			tweet._json.pop('user.profile_background_color', None)
			tweet._json.pop('user.profile_background_image_url', None)
			tweet._json.pop('user.profile_background_image_url_https', None)
			tweet._json.pop('user.profile_background_tile', None)
			tweet._json.pop('user.profile_banner_url', None)
			tweet._json.pop('user.profile_image_url', None)
			tweet._json.pop('user.profile_image_url_https', None)
			tweet._json.pop('user.profile_link_color', None)
			tweet._json.pop('user.profile_location', None)
			tweet._json.pop('user.profile_sidebar_border_color', None)
			tweet._json.pop('user.profile_sidebar_fill_color', None)
			tweet._json.pop('user.profile_text_color', None)
			tweet._json.pop('user.profile_use_background_image', None)                        
			#tweet._json.pop('user', None)
			tweet._json['source'] = re.sub("<.*?>", "",tweet._json['source'])
			tweet._json['source'] = re.sub("\"", "",tweet._json['source'])
			bod = bod + json.dumps(tweet._json) + "\n"
		try:
			client.bulk(index="tw" + sys.argv[1],doc_type="tweets",body=bod)
		except (RuntimeError, TypeError, NameError):
			elog(screen_name, "failed bulk add " + str(len(bod)))
			pass
		bod = ""
		oldest = new_tweets[-1].id - 1
		#wasting 200 calls? How do this efficiently? Count tweets processed?
		new_tweets = api.user_timeline(screen_name = screen_name,count=810, max_id=tweet.id)
		if(oldest == new_tweets[-1].id - 1):
			new_tweets = []
		#elog("added " + screen_name + " " + str(cnt))

# get_all_tweets
def check_resources():
	# make sure prior instance has exited
	for p in psutil.process_iter(attrs=['name', 'username']):
		if (p.info['username'] == getpass.getuser()) and (p.info['name'] == "work" +getpass.getuser()):
			elog("FAIL", "already running")
			sys.exit()

	setproctitle.setproctitle("work" + getpass.getuser())

	i = 0
	while (i < 30):
		zcpu = psutil.cpu_percent(interval=None, percpu=False)
		zmem = psutil.virtual_memory()[2]
		if (zcpu < 75) and (zmem < 75):
			i = 60
		i = i + 2
		time.sleep(2)

	#i < 60, don't have resources
	if (i  < 60):
		print()
		elog("FAIL","zcpu " + str(zcpu) + " zmem " + str(zmem))
		sys.exit()
	else:
		print("")
		#elog(getpass.getuser(),"sufficient resources to run")

# check_resources

# main begin
config = configparser.ConfigParser()
config.read(os.environ['HOME'] +'/.twitter')

try:
	ssl_context = create_ssl_context(cafile='/home/root-ca.pem')
	ssl_context.check_hostname = False
	ssl_context.verify_mode = ssl.CERT_NONE

	client = Elasticsearch('https://hp0.netwarsystem.com:9200', ssl_context=ssl_context, timeout=60, http_auth=("admin", "LongPassword2019"))
except:
	sys.exit()

#ensure prior instance has finished, and CPU/MEM usage < 75%
check_resources()


auth = tweepy.OAuthHandler(config['API']['consumer_key'], config['API']['consumer_secret'])
auth.set_access_token(config['API']['access_token_key'], config['API']['access_token_secret'])
api = tweepy.API(auth)
my = api.verify_credentials()
#listlim = api.rate_limit_status()[u'resources'][u'lists'][u'/lists/members'][u'remaining']
#tweetlim = api.rate_limit_status()[u'resources'][u'statuses'][u'/statuses/user_timeline'][u'remaining']
#elog("LIMITS"," tweetlim " + str(tweetlim) + " listlim " + str(listlim))

wal = Walrus(host='localhost', port=6379, db=0)
work = wal.Set("work")
nextacct = work.pop().decode('utf-8')
print("NEXT NEXT NEXT " + nextacct)
elog(nextacct," starting")
get_all_tweets(nextacct)
print("got all tweets")
try:
	y = api.get_user(nextacct)
	if y:
		twbod = "{ \"index\" : { \"_index\" : \"tu" + sys.argv[1] + "\", \"_type\" : \"twid\", \"_id\" : \"" + y.id_str + "\"} }\n"
		twbod = twbod + json.dumps(y._json)
		client.bulk(index="tu" + sys.argv[1],doc_type="twid",body=twbod)
		elog(nextacct," tulogging")

		twbod = "{ \"index\" : { \"_index\" : \"userids\", \"_type\" : \"userid\", \"_id\" : \"" + y.id_str + "\" }\n"
		y = squish.squishuser(y)
		twbod = twbod + json.dumps(y._json)
		client.bulk(index="userids",doc_type="twid",body=twbod)
		elog(nextacct," userids")
except (RuntimeError, TypeError, NameError):
	elog(nextacct, " tulogging failed")
	pass

print("logged " + nextacct)
