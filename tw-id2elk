#!/usr/bin/python
# one shot bulk Twitter account recorder
import csv, json, os, random, re, sys, time, datetime
import urllib2, redis, tweepy
import psutil, getpass, setproctitle, platform
import logging, logging.handlers
from simpleconfigparser import simpleconfigparser
from bs4 import BeautifulSoup
from time import gmtime, strftime
from elasticsearch import Elasticsearch
from elasticsearch_dsl import Search
from walrus import *

def elog(acct, mesg):
        mydate = str(datetime.datetime.utcnow())
        ebod = "{ \"index\" : { \"_index\" : \"perflog\", \"_type\" : \"perfdata\"} }\n"
        ebod = ebod + "{\"host\" : \"" + platform.node() + "\", \"name\" : \"" + acct + "\", \"event\" : \"" + mesg + "\" , \"date\" : \"" + mydate + "\"}\n"
        try:
                client.bulk(body=ebod)
        except(RuntimeError, TypeError, NameError):
                pass

def get_all_tweets(screen_name):
	cnt = 0
	bod = ""
	new_tweets = api.user_timeline(screen_name = screen_name,count=20)
	while(len(new_tweets) > 0):
		for tweet in new_tweets:
			cnt = cnt + 1
			#print(tweet.id)
			bod = bod + "{ \"index\" : { \"_index\" : \"tweets" + getpass.getuser() + "\", \"_type\" : \"tweets\" } }\n"
			bod = bod + json.dumps(tweet._json) + "\n"
		try:
			client.bulk(index="tweets" + getpass.getuser(),doc_type="tweets",body=bod)
			elog(getpass.getuser(), "successful bulk add " + str(len(bod)))
		except (RuntimeError, TypeError, NameError):
			elog(getpass.getuser(), "failed bulk add " + str(len(bod)))
			pass
		bod = ""
		oldest = new_tweets[-1].id - 1
		#wasting 200 calls? How do this efficiently? Count tweets processed?
		new_tweets = api.user_timeline(screen_name = screen_name,count=200, max_id=tweet.id)
		if(oldest == new_tweets[-1].id - 1):
			new_tweets = []
		elog(getpass.getuser(), "added " + screen_name + " " + str(cnt))

# get_all_tweets
def check_resources():
	# make sure prior instance has exited
	for p in psutil.process_iter(attrs=['name', 'username']):
        	if (p.info['username'] == getpass.getuser()) and (p.info['name'] == "work" +getpass.getuser()):
                	elog(getpass.getuser(), "already running")
			sys.exit()

	setproctitle.setproctitle("work" + getpass.getuser())
	time.sleep(random.random() * 20)

	i = 0
	while (i < 30):
		zcpu = psutil.cpu_percent(interval=None, percpu=False)
		zmem = psutil.virtual_memory()[2]
		if (zcpu < 75) and (zmem < 75):
			i = 60
		i = i + 2
		time.sleep(2)

	#i < 60, don't have resources
        if (i  < 60):
                print()
                elog(getpass.getuser(),"zcpu " + str(zcpu) + " zmem " + str(zmem))
                sys.exit()
        else:
		print("")
                #elog(getpass.getuser(),"sufficient resources to run")

# check_resources

# main begin
config = simpleconfigparser()
config.read(os.environ['HOME'] +'/.twitter')

try:
	client = Elasticsearch(use_ssl=True, verify_certs=False, http_auth="admin:admin")
except:
	sys.exit()

#ensure prior instance has finished, and CPU/MEM usage < 75%
check_resources()


auth = tweepy.OAuthHandler(config.API.consumer_key, config.API.consumer_secret)
auth.set_access_token(config.API.access_token_key, config.API.access_token_secret)
api = tweepy.API(auth)
my = api.verify_credentials()
listlim = api.rate_limit_status()[u'resources'][u'lists'][u'/lists/members'][u'remaining']
tweetlim = api.rate_limit_status()[u'resources'][u'statuses'][u'/statuses/user_timeline'][u'remaining']
elog(getpass.getuser(), " tweetlim " + str(tweetlim) + " listlim " + str(listlim))

wal = Walrus(host='localhost', port=6379, db=0)
work = wal.Set(getpass.getuser() + "work")
nextacct = work.pop()
elog(getpass.getuser(), "starting on " + nextacct)
get_all_tweets(nextacct)

