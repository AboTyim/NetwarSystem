#!/usr/bin/python
import csv, json, os, random, re, sys, time
import urllib2, redis, tweepy
import psutil, getpass, setproctitle
import logging, logging.handlers
from simpleconfigparser import simpleconfigparser
from bs4 import BeautifulSoup
from time import gmtime, strftime
from elasticsearch import Elasticsearch
from elasticsearch_dsl import Search
from walrus import *

def load_lists():
	y = api.verify_credentials()

	lists = api.lists_all(y.screen_name)
	for list in lists:
		print(list.slug)
		if re.match("r-", list.slug):
			for member in tweepy.Cursor(api.list_members,name,list.slug).items():
				try:
					# see if its in cache
					y = red.get(member.screen_name)
					if not y:
						#never seen before
						z = api.get_user(member.screen_name)
						if z:
							red.set(z.screen_name,z.id_str)
							red.set(z.id_str,json.dumps(z._json))
				except (RuntimeError, TypeError, NameError):
					pass
# load_lists

def get_all_tweets(screen_name):
	new_tweets = api.user_timeline(screen_name = screen_name,count=20)
	while(len(new_tweets) > 0):
		for tweet in new_tweets:
			bod = "{ \"index\" : { \"_index\" : \"test\", \"_type\" : \"tweets\" } }\n"
			bod = bod + json.dumps(tweet._json) + "\n"
			try:
				client.bulk(index="test",doc_type="tweets",body=bod)
			except (RuntimeError, TypeError, NameError):
				pass
		print(tweet.id)
		oldest = new_tweets[-1].id - 1
		#wasting 200 calls? How do this efficiently? Count tweets processed?
		new_tweets = api.user_timeline(screen_name = screen_name,count=200, max_id=tweet.id)
		if(oldest == new_tweets[-1].id - 1):
			new_tweets = []
	print(tweet.id)

# get_all_tweets
	

def twalive(acctname):
	#Account does not exist, or account has self-suspended, no way to tell w/o
	#trying to rename another account into the name in question
	try:
		urllib2.urlopen("http://twitter.com/" + acctname)
	except urllib2.URLError:
		return("renamed")

	#Account has been suspended by Twitter - this misses some jihadi accounts maybe UTF-8 again?
	soup = BeautifulSoup(urllib2.urlopen("http://twitter.com/" + acctname), "html5lib")
	for thing in soup("title"):
		if(thing.text == "Twitter / Account Suspended"):
			return("suspended")
	
	#This is the account lookup that gets overloaded, leading to spurious idling
	lookups = api.rate_limit_status()[u'resources'][u'users'][u'/users/show/:id'][u'remaining']
	if (lookups < 10):
		return("API exhausted")
	
	#Account is protected, friendship checks in tweepy just don't work
	#so try to get one tweet to see if we are following the target
	y = api.get_user(acctname)
	if y.protected:
		try:
			cnt = len(api.user_timeline(screen_name = acctname,count=1))
		except tweepy.TweepError:
			return("protected")
	cnt = len(api.user_timeline(screen_name = acctname,count=1))
	if(cnt == 0):
		return("empty")

	return("accessible")
# twalive

def check_resources():
	# make sure prior instance has exited
	for p in psutil.process_iter(attrs=['name', 'username']):
        	if (p.info['username'] == getpass.getuser()) and (p.info['name'] == "trmqueue"):
                	print("already running")
			sys.exit()

	setproctitle.setproctitle("trmqueue")
	#time.sleep(random.random() * 20)

	i = 0
	while (i < 30):
		zcpu = psutil.cpu_percent(interval=None, percpu=False)
		zmem = psutil.virtual_memory()[2]
		if (zcpu < 75) and (zmem < 75):
			i = 60
		i = i + 2
		time.sleep(2)

	#i < 60, don't have resources
	if (i  < 60):
		print("zcpu " + str(zcpu) + " zmem " + str(zmem))
		print("not running due to resource limits")
		sys.exit()

# check_resources
# main begin

# load config
config = simpleconfigparser()
config.read(os.environ['HOME'] +'/.twitter')

# logging will always be used
ml = logging.getLogger('MyLogger')
ml.setLevel(logging.DEBUG)
handler = logging.handlers.SysLogHandler(address=('127.0.0.1', 555), facility=logging.handlers.SysLogHandler.LOG_DAEMON)
ml.addHandler(handler)
print("logging started")

#ensure prior instance has finished, and CPU/MEM usage < 75%
check_resources()
print("resources good")

auth = tweepy.OAuthHandler(config.API.consumer_key, config.API.consumer_secret)
auth.set_access_token(config.API.access_token_key, config.API.access_token_secret)
api = tweepy.API(auth)
print("API auth")

#red = redis.StrictRedis(host='localhost', port=6379, db=0)
red = Walrus(host='localhost', port=6379, db=0)
print("Walrus connected")

load_lists()
print("lists loaded")

# check twitter API
listlim = api.rate_limit_status()[u'resources'][u'lists'][u'/lists/members'][u'remaining']
tweetlim = api.rate_limit_status()[u'resources'][u'statuses'][u'/statuses/user_timeline'][u'remaining']

print("listlim " + str(listlim) + " tweetlim " + str(tweetlim))

# list semantics go here

if (twalive(sys.argv[1]) == "accessible"):
	# don't bug Elastic till we're sure we've got data to load
	client = Elasticsearch()
	print(sys.argv[1])
	get_all_tweets(sys.argv[1])
